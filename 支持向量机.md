# 支持向量机简介
支持向量机（SVM）是一种有监督分类算法，主要用于对付小样本数据。对于大数据，SVM难以训练，存在较大弊端，对于大数据，使用神经网络结果更优。
## SVM知识点
### 1.1支持向量（支撑向量）

支持起超平面的样本点即为支持向量
### 1.2核函数

本质上为一种非线性映射函数，目的是将输入空间映射到高维特征空间，从而将在低维线性不可分的模式转换成高维线性可分的模式，同时核函数将m维高维空间的内积运算转换成n维低维输入空间的核函数计算，从而避免了‘维数灾难’，大大简化了计算。
### 1.3SVM常用参数
#### 1.3.1 常用核函数
+ linear 
+ Polunamial
+ Gaussian（RBF)
+ Sigmoid
  
(1).线性核函数：即不使用核函数，当不采用非常复杂的函数，或者我们的训练特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。

(2).多项式核函数：有三个参数设置，degree：多项式核函数的阶数，默认是3；-g用来设置和函数中的grmma参数设置，和RBF作用相同，默认是1/K（K为样本个数）；-r用来设置核函数中coef0（类似于b），默认值是0

(3).径向基核函数：应用最广泛，有一个参数 -g 用来设置核函数中的gamma参数设置，默认值是1/K（K是类别数）

(4).sigmoid核函数：
有两个参数：-g用来设置核函数中的gamma参数设置，也就是公式中的第一个r（gamma），默认值是1/K（k是类别数），-r用来设置核函数中的coef0。

#### 1.3.2惩罚项参数C和gamma参数作用
SVM模型有两个非常重要的参数C与gamma。其中 C是惩罚系数，即对误差的宽容度。c越高，说明越不能容忍出现误差,容易过拟合。C越小，容易欠拟合。C过大或过小，泛化能力变差
 gamma是选择RBF函数作为kernel后，该函数自带的一个参数。隐含地决定了数据映射到新的特征空间后的分布，gamma越大，支持向量越少，gamma值越小，支持向量越多。支持向量的个数影响训练与预测的速度。
 
 RBF公式里面的sigma和gamma的关系如下：![](http://img.blog.csdn.net/20150606105930104?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbHVqaWFuZG9uZzE=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
 
 
这里面大家需要注意的就是gamma的物理意义，大家提到很多的RBF的幅宽，它会影响每个支持向量对应的高斯的作用范围，从而影响泛化性能。我的理解：如果gamma设的太大，会很小，很小的高斯分布(正态分布)长得又高又瘦， 会造成只会作用于支持向量样本附近，对于未知样本分类效果很差，存在训练准确率可以很高，(如果让无穷小，则理论上，高斯核的SVM可以拟合任何非线性数据，但容易过拟合)而测试准确率不高的可能，就是通常说的过训练；而如果设的过小，则会造成平滑效应太大，无法在训练集上得到特别高的准确率，也会影响测试集的准确率。

 
此外，可以明确的两个结论是：
+ 结论1：样本数目少于特征维度并不一定会导致过拟合。
 + 结论2：RBF核应该可以得到与线性核相近的效果（按照理论，RBF核可以模拟线性核），可能好于线性核，也可能差于，但是，不应该相差太多。
当然，很多问题中，比如维度过高，或者样本海量的情况下，大家更倾向于用线性核，因为效果相当，但是在速度和模型大小方面，线性核会有更好的表现。


++须知rbf实际是记忆了若干样例，在sv中各维权重重要性等同。线性核学出的权重是feature weighting作用或特征选择 。++

#### 1.3.3 Sklearn.svm.SVC

+ C：惩罚项，float类型，可选参数，默认为1.0，C越大，即对分错样本的惩罚程度越大，因此在训练样本中准确率越高，但是泛化能力降低，也就是对测试数据的分类准确率降低。相反，减小C的话，容许训练样本中有一些误分类错误样本，泛化能力强。对于训练样本带有噪声的情况，一般采用后者，把训练样本集中错误分类的样本作为噪声。
+ kernel：核函数类型，str类型，默认为’rbf’。可选参数为：
   + ’linear’：线性核函数
   + ‘poly’：多项式核函数
   + ‘rbf’：径像核函数/高斯核
   + ‘sigmod’：sigmod核函数
   + ‘precomputed’：核矩阵
   + precomputed表示自己提前计算好核函数矩阵，这时候算法内部就不再用核函数去计算核矩阵，而是直接用你给的核矩阵，核矩阵需要为n*n的。
+ degree：多项式核函数的阶数，int类型，可选参数，默认为3。这个参数只对多项式核函数有用，是指多项式核函数的阶数n，如果给的核函数参数是其他核函数，则会自动忽略该参数。
+ gamma：核函数系数，float类型，可选参数，默认为auto。只对’rbf’ ,’poly’ ,’sigmod’有效。如果gamma为auto，代表其值为样本特征数的倒数，即1/n_features。
+ coef0：核函数中的独立项，float类型，可选参数，默认为0.0。只有对’poly’ 和,’sigmod’核函数有用，是指其中的参数c。
+ probability：是否启用概率估计，bool类型，可选参数，默认为False，这必须在调用fit()之前启用，并且会fit()方法速度变慢。
+ shrinking：是否采用启发式收缩方式，bool类型，可选参数，默认为True。
+ tol：svm停止训练的误差精度，float类型，可选参数，默认为1e^-3。
+ cache_size：内存大小，float类型，可选参数，默认为200。指定训练所需要的内存，以MB为单位，默认为200MB。
+ class_weight：类别权重，dict类型或str类型，可选参数，默认为None。给每个类别分别设置不同的惩罚参数C，如果没有给，则会给所有类别都给C=1，即前面参数指出的参数C。如果给定参数’balance’，则使用y的值自动调整与输入数据中的类频率成反比的权重。
+ verbose：是否启用详细输出，bool类型，默认为False，此设置利用libsvm中的每个进程运行时设置，如果启用，可能无法在多线程上下文中正常工作。一般情况都设为False，不用管它。
max_iter：最大迭代次数，int类型，默认为-1，表示不限制。
+ decision_function_shape：决策函数类型，可选参数’ovo’和’ovr’，默认为’ovr’。’ovo’表示one vs one，’ovr’表示one vs rest。
+ random_state：数据洗牌时的种子值，int类型，可选参数，默认为None。伪随机数发生器的种子,在混洗数据时用于概率估计。


## SVM的优缺点
优点
+ 可用于线性/非线性分类，也可以用于回归，泛化错误率低，也就是说具有良好的学习能力，且学到的结果具有很好的推广性。
+ 可以解决小样本情况下的机器学习问题，可以解决高维问题，可以避免神经网络结构选择和局部极小点问题。
+ SVM是最好的现成的分类器，现成是指不加修改可直接使用。并且能够得到较低的错误率，SVM可以对训练集之外的数据点做很好的分类决策。

缺点
+ 对参数调节和和函数的选择敏感。